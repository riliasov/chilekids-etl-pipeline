#!/usr/bin/env python3
"""
–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π ELT-–ø—Ä–æ—Ü–µ—Å—Å —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–µ—à–∞:
1. –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –∏–∑ raw.source_events
2. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤ staging.records
3. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–µ–∂–∏–º --test –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python -m scripts.run_elt          # –ü–æ–ª–Ω—ã–π –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫
    python -m scripts.run_elt --test   # –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º (–ø–µ—Ä–≤—ã–µ 100 –∑–∞–ø–∏—Å–µ–π, –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã)
"""
import sys
import asyncio
import argparse
import logging
import json
import time
from typing import List, Dict, Any

from src.transform.normalizer import get_changed_raw_records, normalize_record
from src.transform.loader import upsert_staging_records_batch
from src.utils.db import init_db_pool, close_db_pool
from src.utils.config import settings

# Configure logging
logging.basicConfig(
    level=getattr(logging, settings.LOG_LEVEL),
    format='%(asctime)s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


async def run_incremental_elt(test_mode: bool = False):
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π ELT: —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö raw-–∑–∞–ø–∏—Å–µ–π –≤ staging.
    
    Args:
        test_mode: –ï—Å–ª–∏ True, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 100 –∑–∞–ø–∏—Å–µ–π –∏ –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã
    """
    await init_db_pool()
    
    try:
        # Determine processing limits
        limit = settings.TEST_LIMIT if test_mode else None
        batch_size = settings.BATCH_SIZE
        
        mode_str = "–¢–ï–°–¢–û–í–´–ô" if test_mode else "–ü–û–õ–ù–´–ô"
        logger.info(f"üöÄ === {mode_str} ELT –ü–†–û–¶–ï–°–° ===")
        logger.info(f"–ü–∞–∫–µ—Ç: {batch_size}, –õ–∏–º–∏—Ç: {limit or '–ù–µ—Ç'}")
        
        start_time = time.time()
        
        # Step 1: Query changed/new records from raw
        logger.info("üîç 1. –ü–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ raw.source_events...")
        query_start = time.time()
        raw_records = await get_changed_raw_records(limit=limit)
        query_duration = time.time() - query_start
        
        if not raw_records:
            logger.info("üí§ –ù–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
            return
        
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(raw_records)} (–ø–æ–∏—Å–∫ –∑–∞–Ω—è–ª {query_duration:.1f}—Å)")
        
        # Step 2: Normalize records
        logger.info("üõ†Ô∏è 2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
        norm_start = time.time()
        normalized_records: List[Dict[str, Any]] = []
        errors = 0
        
        for idx, raw_rec in enumerate(raw_records):
            try:
                normalized = normalize_record(
                    raw_id=raw_rec['raw_id'],
                    sheet_row_number=raw_rec.get('sheet_row_number'),
                    received_at=raw_rec['received_at'],
                    payload=raw_rec['raw_payload']
                )
                normalized_records.append(normalized)
                
            except Exception as e:
                errors += 1
                # Log errors only if critical or in debug
                if errors <= 5: # Show first 5 errors only to keep log compact
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (ID={raw_rec.get('raw_id')}): {e}")
                continue
        
        norm_duration = time.time() - norm_start
        logger.info(
            f"‚ú® –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: {len(normalized_records)} "
            f"(–æ—à–∏–±–æ–∫: {errors}) –∑–∞ {norm_duration:.1f}—Å"
        )

        # Monitoring: Check error rate
        total_processed = len(raw_records)
        if total_processed > 0:
            error_rate = errors / total_processed
            if error_rate > 0.1:  # 10% threshold
                logger.warning(
                    f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫! "
                    f"{error_rate:.1%} ({errors}/{total_processed})."
                )
        
        # Step 3: Show examples in test mode
        if test_mode and normalized_records:
            logger.info("--- –ü–†–ò–ú–ï–†–´ –ó–ê–ü–ò–°–ï–ô (–ø–µ—Ä–≤—ã–µ 3) ---")
            
            for i, rec in enumerate(normalized_records[:3], 1):
                logger.info(f"–ó–∞–ø–∏—Å—å {i}: {rec.get('client')} | {rec.get('total_rub')} —Ä—É–±. | {rec.get('category')}")
        
        # Step 4: Upsert to staging
        upsert_start = time.time()
        upserted_count = 0
        if normalized_records:
            logger.info(f"üíæ 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(normalized_records)} –∑–∞–ø–∏—Å–µ–π –≤ –ë–î...")
            upserted_count = await upsert_staging_records_batch(
                normalized_records,
                batch_size=batch_size
            )
            logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {upserted_count}")
        else:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")
        upsert_duration = time.time() - upsert_start
        
        total_duration = time.time() - start_time
        
        # Summary
        logger.info("üìä === –ò–¢–û–ì–ò ===")
        logger.info(f"–í—Ä–µ–º—è: {total_duration:.1f}—Å | –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(raw_records)} | –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {upserted_count}")
        logger.info(f"–≠—Ç–∞–ø—ã (—Å–µ–∫): –ü–æ–∏—Å–∫={query_duration:.1f}, –ù–æ—Ä–º={norm_duration:.1f}, –°–æ—Ö—Ä={upsert_duration:.1f}")
        logger.info("=========================")
        
    except Exception as e:
        logger.error(f"ELT process failed: {e}", exc_info=True)
        raise
    
    finally:
        await close_db_pool()


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ CLI."""
    parser = argparse.ArgumentParser(
        description="Incremental ELT: raw.source_events ‚Üí staging.records"
    )
    parser.add_argument(
        '--test',
        action='store_true',
        help='Test mode: process only first 100 records and show examples'
    )
    
    args = parser.parse_args()
    
    try:
        asyncio.run(run_incremental_elt(test_mode=args.test))
    except KeyboardInterrupt:
        logger.info("Process interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()

