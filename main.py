#!/usr/bin/env python3
"""
–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π ELT-–ø—Ä–æ—Ü–µ—Å—Å —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–µ—à–∞:
1. –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –∏–∑ raw.source_events
2. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤ staging.records
3. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–µ–∂–∏–º --test –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python main.py run          # –ü–æ–ª–Ω—ã–π –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫
    python main.py run --test   # –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º (–ø–µ—Ä–≤—ã–µ 100 –∑–∞–ø–∏—Å–µ–π, –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã)
    python main.py load <SPREADSHEET_ID> [RANGE]  # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ Google Sheets
    python main.py check        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ–∫—Ä—É–∂–µ–Ω–∏–µ
"""
import sys
import asyncio
import argparse
import logging
import json
import time
from typing import List, Dict, Any

from src.transform import get_changed_raw_records, normalize_record, upsert_staging_records_batch
from src.db import init_db_pool, close_db_pool, fetch
from src.config import settings
from src.sheets import fetch_google_sheets
from src.logger import setup_logging


logger = logging.getLogger(__name__)

# --- Command: RUN ---

async def run_incremental_elt(test_mode: bool = False, source: str = 'google_sheets', source_type: str = 'live'):
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π ELT: —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö raw-–∑–∞–ø–∏—Å–µ–π –≤ staging.
    
    Args:
        test_mode: –ï—Å–ª–∏ True, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 100 –∑–∞–ø–∏—Å–µ–π –∏ –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã
    """
    await init_db_pool()
    
    try:
        # Determine processing limits
        limit = settings.TEST_LIMIT if test_mode else None
        batch_size = settings.BATCH_SIZE
        
        mode_str = "–¢–ï–°–¢–û–í–´–ô" if test_mode else "–ü–û–õ–ù–´–ô"
        logger.info(f"üöÄ === {mode_str} ELT –ü–†–û–¶–ï–°–° ===")
        logger.info(f"–ü–∞–∫–µ—Ç: {batch_size}, –õ–∏–º–∏—Ç: {limit or '–ù–µ—Ç'}")
        
        start_time = time.time()
        
        # Step 1: Query changed/new records from raw
        logger.info(f"üîç 1. –ü–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ raw.source_events (source={source})...")
        query_start = time.time()
        raw_records = await get_changed_raw_records(source=source, limit=limit)
        query_duration = time.time() - query_start
        
        if not raw_records:
            logger.info("üí§ –ù–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
            return
        
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(raw_records)} (–ø–æ–∏—Å–∫ –∑–∞–Ω—è–ª {query_duration:.1f}—Å)")
        
        # Step 2: Normalize records
        logger.info("üõ†Ô∏è 2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
        norm_start = time.time()
        normalized_records: List[Dict[str, Any]] = []
        errors = 0
        
        for idx, raw_rec in enumerate(raw_records):
            try:
                normalized = normalize_record(
                    raw_id=raw_rec['raw_id'],
                    sheet_row_number=raw_rec.get('sheet_row_number'),
                    received_at=raw_rec['received_at'],
                    payload=raw_rec['raw_payload'],
                    source_type=source_type
                )
                normalized_records.append(normalized)
                
            except Exception as e:
                errors += 1
                # Log errors only if critical or in debug
                if errors <= 5: # Show first 5 errors only to keep log compact
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (ID={raw_rec.get('raw_id')}): {e}")
                continue
        
        norm_duration = time.time() - norm_start
        logger.info(
            f"‚ú® –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: {len(normalized_records)} "
            f"(–æ—à–∏–±–æ–∫: {errors}) –∑–∞ {norm_duration:.1f}—Å"
        )

        # Monitoring: Check error rate
        total_processed = len(raw_records)
        if total_processed > 0:
            error_rate = errors / total_processed
            if error_rate > 0.1:  # 10% threshold
                logger.warning(
                    f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫! "
                    f"{error_rate:.1%} ({errors}/{total_processed})."
                )
        
        # Step 3: Show examples in test mode
        if test_mode and normalized_records:
            logger.info("--- –ü–†–ò–ú–ï–†–´ –ó–ê–ü–ò–°–ï–ô (–ø–µ—Ä–≤—ã–µ 3) ---")
            
            for i, rec in enumerate(normalized_records[:3], 1):
                logger.info(f"–ó–∞–ø–∏—Å—å {i}: {rec.get('client')} | {rec.get('total_rub')} —Ä—É–±. | {rec.get('category')}")
                
        # Step 4: Upsert to staging
        upsert_start = time.time()
        upserted_count = 0
        if normalized_records:
            logger.info(f"üíæ 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(normalized_records)} –∑–∞–ø–∏—Å–µ–π –≤ –ë–î...")
            upserted_count = await upsert_staging_records_batch(
                normalized_records,
                batch_size=batch_size
            )
            logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {upserted_count}")
        else:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")
        upsert_duration = time.time() - upsert_start
        
        total_duration = time.time() - start_time
        
        # Summary
        logger.info("üìä === –ò–¢–û–ì–ò ===")
        logger.info(f"–í—Ä–µ–º—è: {total_duration:.1f}—Å | –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(raw_records)} | –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {upserted_count}")
        logger.info(f"–≠—Ç–∞–ø—ã (—Å–µ–∫): –ü–æ–∏—Å–∫={query_duration:.1f}, –ù–æ—Ä–º={norm_duration:.1f}, –°–æ—Ö—Ä={upsert_duration:.1f}")
        logger.info("=========================")
        
    except Exception as e:
        logger.error(f"ELT process failed: {e}", exc_info=True)
        raise
    
    finally:
        await close_db_pool()


async def load_raw(source: str, records: List[Dict[str, Any]]):
    """Bulk insert raw records into raw.data table."""
    if not records:
        return
    
    pool = await init_db_pool()
    async with pool.acquire() as conn:
        async with conn.transaction():
            await conn.executemany(
                "INSERT INTO raw.data (id, source, payload, payload_hash) VALUES ($1, $2, $3, $4) ON CONFLICT (id) DO NOTHING",
                [
                    (
                        r['id'],
                        source,
                        json.dumps(r['payload'], ensure_ascii=False),
                        __import__('hashlib').md5(
                            json.dumps(r['payload'], sort_keys=True).encode()
                        ).hexdigest()
                    )
                    for r in records
                ]
            )


async def run_load_sheets(spreadsheet_id: str, range_name: str, source: str = 'google_sheets'):
    """Load data from Google Sheets into raw.data."""
    await init_db_pool()
    try:
        logger.info(f"üì• –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ Google Sheets: {spreadsheet_id} {range_name} (source={source}) ...")
        records = await fetch_google_sheets(spreadsheet_id, range_name)
        logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(records)} —Å—Ç—Ä–æ–∫. –ó–∞–≥—Ä—É–∑–∫–∞ –≤ raw.data ...")
        
        # Prepare rows with duplicate detection
        rows = []
        seen_hashes = {}
        duplicates_count = 0
        
        for i, r in enumerate(records):
            # 1. Try to get explicit ID
            # Normalize keys to find 'id' case-insensitively
            keys_norm = {k.lower().strip(): k for k in r.keys()}
            id_key = keys_norm.get('pk') or keys_norm.get('id') or keys_norm.get('row_id') or keys_norm.get('uuid')
            
            raw_id = None
            if id_key and r[id_key]:
                raw_id = str(r[id_key]).strip()
            
            # 2. Fallback to Content Hash
            import hashlib
            payload_str = json.dumps(r, sort_keys=True)
            h = hashlib.sha256(payload_str.encode('utf-8')).hexdigest()
            
            if not raw_id:
                # User warning logic: Check for full duplicates in source
                if h in seen_hashes:
                    duplicates_count += 1
                    if duplicates_count <= 5:
                        logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞-–¥—É–±–ª–∏–∫–∞—Ç (—Å—Ç—Ä–æ–∫–∞ {i+2}). –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID. Content hash: {h[:8]}")
                seen_hashes[h] = True
                
                # We still need a unique ID for DB constraints, so we use hash + row info as fallback
                # But heavily encourage PK usage in logs
                raw_id = f"gsheet_auto_{h[:12]}_{i}"

            rows.append({
                'id': raw_id,
                'payload': r
            })
        
        if duplicates_count > 0:
             logger.warning(f"‚ö†Ô∏è –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ö–µ—à–µ–π –¥–∞–Ω–Ω—ã—Ö: {duplicates_count}. –≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø—Ä–æ–±–ª–µ–º–∞–º. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –∫–æ–ª–æ–Ω–∫—É 'id' –≤ Google Sheet.")

        await load_raw(source, rows)
        logger.info(f"üíæ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(rows)} —Å—Ç—Ä–æ–∫.")
    finally:
        await close_db_pool()


async def run_check_env():
    """Check environment, .env, and DB connection."""
    logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è...")
    
    import os
    if not os.path.exists('.env'):
        logger.error("‚ùå .env not found")
    else:
        logger.info("‚úÖ .env found")
    
    if not settings.POSTGRES_URI:
        logger.error("‚ùå POSTGRES_URI not set")
    else:
        logger.info("‚úÖ POSTGRES_URI set")
    
    try:
        await init_db_pool()
        res = await fetch("SELECT 1 as val")
        if res and res[0]['val'] == 1:
            logger.info("‚úÖ DB Connection successful")
        else:
            logger.error("‚ùå DB Connection failed")
    except Exception as e:
        logger.error(f"‚ùå DB Connection failed: {e}")
    finally:
        await close_db_pool()



def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ CLI."""
    parser = argparse.ArgumentParser(
        description="ChileKids ETL Pipeline: raw.source_events ‚Üí staging.records"
    )
    parser.add_argument("--debug", action="store_true", help="Set log level to DEBUG")
    parser.add_argument("--json-logs", action="store_true", help="Enable JSON logging format")

    subparsers = parser.add_subparsers(dest="command", required=True)
    
    # Run command
    p_run = subparsers.add_parser('run', help='Run incremental ELT')
    p_run.add_argument(
        '--test',
        action='store_true',
        help='Test mode: process only first 100 records and show examples'
    )
    p_run.add_argument("--source", default="google_sheets", help="Raw data source name")
    p_run.add_argument("--source-type", default="live", help="Target staging source_type tag")
    
    # Load command
    p_load = subparsers.add_parser('load', help='Load from Google Sheets')
    p_load.add_argument('spreadsheet_id', help='Google Spreadsheet ID')
    p_load.add_argument('range', nargs='?', default='Sheet1!A:AF', help='Range (default: Sheet1!A:AF)')
    p_load.add_argument('--source', default='google_sheets', help='Store as this source in raw.data')
    
    # Check command
    p_check = subparsers.add_parser('check', help='Check environment')
    
    args = parser.parse_args()
    
    # Configure logging based on args
    log_level = "DEBUG" if getattr(args, 'debug', False) else settings.LOG_LEVEL
    json_format = getattr(args, 'json_logs', False)
    setup_logging(level=log_level, json_format=json_format)
    
    try:
        if args.command == 'run':
            asyncio.run(run_incremental_elt(test_mode=args.test, source=args.source, source_type=args.source_type))
        elif args.command == 'load':
            asyncio.run(run_load_sheets(args.spreadsheet_id, args.range, source=args.source))
        elif args.command == 'check':
            asyncio.run(run_check_env())
    except KeyboardInterrupt:
        logger.info("Process interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
